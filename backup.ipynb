{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.special import gamma, polygamma\n",
    "\n",
    "import sympy\n",
    "from sympy.stats import Beta, density, E, variance\n",
    "from sympy import Symbol, simplify, pprint, expand_func, S, pi, Matrix, Array, symbols, IndexedBase, Idx, lambdify\n",
    "from sympy import Sum, Integral, IndexedBase, Function\n",
    "from sympy.stats import Expectation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(netx)\n",
    "importlib.reload(dl)\n",
    "\n",
    "model = netx.NaiveCNN('mse').cuda()\n",
    "par_model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "\n",
    "loss_fn = nn.modules.loss.MSELoss()\n",
    "#loss_fn = nn.modules.loss.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.0005)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (imgs, huy) in enumerate(train_loader):\n",
    "        imgs, huy = imgs.cuda().float(), huy.cuda().float()\n",
    "        loss = loss_fn(par_model(imgs, huy[:,1]), torch.round(huy[:,-1]*6))\n",
    "        #loss = loss_fn(par_model(imgs, huy[:,1].float()), torch.round(huy[:,-1]*6).long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * M, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "print_stats(test=False)\n",
    "print_stats(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(y_train, dtype=dtype).cuda()\n",
    "u = torch.tensor(u_train, dtype=dtype).cuda()\n",
    "model = MixModel(K).cuda()\n",
    "par_model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "\n",
    "clipper = ProbClipper()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "yu = torch.stack([y,u], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### logit-normal dist\n",
    "sigma = Symbol(\"sigma\", positive=True)\n",
    "mu = Symbol(\"mu\", positive=True)\n",
    "y = Symbol(\"y\", positive=True)\n",
    "pdf = sympy.exp(-(sympy.log(y/(1-y)) - mu)**2/(2*sigma**2)) * (1/(y*(1-y))) / (sigma*(2*pi)**.5)\n",
    "\n",
    "dP_dy = pdf.diff(y).subs({sigma:1, mu:0})\n",
    "\n",
    "scipy.optimize.broyden2(lambdify(y, dP_dy), [0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### beta distribution\n",
    "a_y = Symbol(\"a\", positive=True)\n",
    "b_y = Symbol(\"b\", positive=True)\n",
    "y = Symbol(\"y\", positive=True)\n",
    "Y = sympy.stats.Beta('Y', a_y, b_y)\n",
    "pdf = density(Y)(y)\n",
    "\n",
    "pdf.diff(b_y)\n",
    "\n",
    "dP_db = pdf.diff(b_y).subs({a_y:1, y:.5})\n",
    "\n",
    "dP_da = pdf.diff(alpha).subs({beta:1, y:.5})\n",
    "\n",
    "scipy.optimize.broyden1(lambdify(b_y, dP_db), [1])\n",
    "\n",
    "np.argmax(stats.beta.pdf(.5, alphas, betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta_2d_pdf(a1,b1,a2,b2):\n",
    "    cuda_to_np = lambda x: x.detach().cpu().numpy()[0]\n",
    "    a1 = cuda_to_np(a1)\n",
    "    a2 = cuda_to_np(a2)\n",
    "    b1 = cuda_to_np(b1)\n",
    "    b2 = cuda_to_np(b2)\n",
    "    \n",
    "    mu_X = stats.beta.mean(a1,b1)\n",
    "    mu_Y = stats.beta.mean(a2,b2)\n",
    "    var_X = stats.beta.var(a1,b1)\n",
    "    var_Y = stats.beta.var(a2,b2)\n",
    "    X = [np.linspace(mu_X[i]-var_X[i]**.5, mu_X[i]+var_X[i]**.5,1000) for i in range(K)]\n",
    "    Y = [np.concatenate([((1 - (X[i] - mu_X[i])**2/var_X[i]) * var_Y[i])**.5 + mu_Y[i], \n",
    "          -((1 - (X[i] - mu_X[i])**2/var_X[i]) * var_Y[i])**.5 + mu_Y[i]]) for i in range(K)]\n",
    "    X = [np.tile(X[i],2) for i in range(K)]\n",
    "    print(X[0].shape, Y[0].shape)\n",
    "    [plt.scatter(X[i],Y[i], marker='.') for i in range(K)];\n",
    "    #[plt.scatter(mu_X[i],mu_Y[i], marker='x') for i in range(K)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "K = 50\n",
    "\n",
    "a_y = IndexedBase(\"a_y\", K, positive=True)\n",
    "b_y = IndexedBase(\"b_y\", K, positive=True)\n",
    "a_u = IndexedBase(\"a_u\", K, positive=True)\n",
    "b_u = IndexedBase(\"b_u\", K, positive=True)\n",
    "P_z = IndexedBase(\"P_z\", K, positive=True)\n",
    "y = Symbol(\"y\", positive=True)\n",
    "u = Symbol(\"u\", positive=True)\n",
    "i = Idx(\"i\", K)\n",
    "k_sym = Idx(\"k\", K)\n",
    "\n",
    "f_YU = density(sympy.stats.Beta('fY', sympy.exp(a_y[i]), sympy.exp(b_y[i])))(y) * density(\n",
    "                sympy.stats.Beta('fU', sympy.exp(a_u[i]), sympy.exp(b_u[i])))(u)\n",
    "f_YU_k = density(sympy.stats.Beta('fY_k', sympy.exp(a_y[k_sym]), sympy.exp(b_y[k_sym])))(y) * density(\n",
    "                sympy.stats.Beta('fU_k', sympy.exp(a_u[k_sym]), sympy.exp(b_u[k_sym])))(u)\n",
    "\n",
    "dP_da_k = P_z[k_sym]*f_YU_k.diff(a_y[k_sym])/Sum(P_z[i]*f_YU, (i,0,K-1))\n",
    "dP_da_k_py = lambdify((a_y, b_y, a_u, b_u, P_z, y, k_sym), dP_da_k)\n",
    "\n",
    "dP_db_k = P_z[k_sym]*f_YU_k.diff(b_y[k_sym])/Sum(P_z[i]*f_YU, (i,0,K-1))\n",
    "dP_db_k_py = lambdify((a_y, b_y, a_u, b_u, P_z, y, k_sym), dP_db_k)\n",
    "\n",
    "dP_dpz_k = f_YU_k/Sum(P_z[i]*f_YU, (i,0,K-1))\n",
    "dP_dpz_k_py = lambdify((a_y, b_y, a_u, b_u, P_z, y, k_sym), dP_dpz_k)\n",
    "\n",
    "log_ll = sympy.log(Sum(P_z[i]*f_YU, (i,0,K-1)))\n",
    "log_ll_py = lambdify((a_y, b_y, a_u, b_u, P_z, y), log_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processInput(k, bounds = (.05,50)):\n",
    "    dP_dpz_k_sum = lambda pz: np.sum(dP_dpz_k_py(alphas, betas, p_z[:k] + [pz] + p_z[k:], y_train, k)) - N\n",
    "    dP_da_k_sum = lambda a: np.sum(dP_da_k_py(alphas[:k] + [a] + alphas[k:], betas, p_z, y_train, k))\n",
    "    dP_db_k_sum = lambda b: np.sum(dP_db_k_py(alphas, betas[:k] + [b] + betas[k:], p_z, y_train, k))\n",
    "    dP_dpz_k_sq = lambda pz: dP_dpz_k_sum(pz)**2\n",
    "    dP_da_k_sq = lambda a: dP_da_k_sum(a)**2\n",
    "    dP_db_k_sq = lambda b: dP_db_k_sum(b)**2\n",
    "\n",
    "    try:                         \n",
    "        a_est = np.exp(scipy.optimize.brentq(dP_da_k_sum))#, a=bounds[0], b=bounds[1], disp=False)\n",
    "    except ValueError:\n",
    "        a_est = np.exp(scipy.optimize.minimize_scalar(dP_da_k_sq).x)#, bounds=bounds, method='Bounded').x\n",
    "    #alphas[k] = scipy.optimize.broyden1(dP_da_k_sum, [alphas[k]])[0]\n",
    "    print(\".\",end=\"\")\n",
    "\n",
    "    try:\n",
    "        b_est = np.exp(scipy.optimize.brentq(dP_db_k_sum))#, a=bounds[0], b=bounds[1], disp=False)\n",
    "    except ValueError:\n",
    "        b_est = np.exp(scipy.optimize.minimize_scalar(dP_db_k_sq).x)#, bounds=bounds, method='Bounded').x\n",
    "    print(\".\",end=\"\")\n",
    "\n",
    "    try:\n",
    "        p_z_est = scipy.optimize.brentq(dP_dpz_k_sum, a=0, b=1, disp=False)\n",
    "    except ValueError:\n",
    "        p_z_est = scipy.optimize.minimize_scalar(dP_dpz_k_sq, bounds=(0, 1), method='Bounded').x\n",
    "    print(\".\",end=\"\")\n",
    "    \n",
    "    return a_est, b_est, p_z_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#initialization\n",
    "alphas = list(np.linspace(.1,6,K))\n",
    "betas = list(np.linspace(6,.1,K))\n",
    "p_z = list(np.ones(K)/K) #mixing coefficients\n",
    "\n",
    "patience = 3\n",
    "tol = .1\n",
    "ll_hist = [-10000]*patience\n",
    "\n",
    "a_est = np.zeros(K)\n",
    "b_est = np.zeros(K)\n",
    "p_z_est = np.zeros(K)\n",
    "\n",
    "for _ in range(iters):\n",
    "    lll = np.mean(log_ll_py(alphas, betas, p_z, y_train))\n",
    "    print(\"Mean Log Likelihood: %.2f\" % lll)\n",
    "    if np.max(lll - ll_hist[-patience:]) < tol:\n",
    "        break\n",
    "    \n",
    "    K_data = Parallel(n_jobs=num_cores)(delayed(processInput)(k) for k in range(K))\n",
    "    alphas, betas, p_z_est = zip(*K_data)\n",
    "    alphas = list(alphas)\n",
    "    betas = list(betas)\n",
    "    p_z = list(p_z_est/sum(p_z_est))\n",
    "    ll_hist = ll_hist + [lll]\n",
    "    \n",
    "ll_hist = ll_hist[patience:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dll_da_k = lambdify((a_y, b_y, a_u, b_u, P_z, y, k_sym), log_ll.diff(a_y[k_sym]), modules='sympy')\n",
    "dll_db_k = lambdify((a_y, b_y, a_u, b_u, P_z, y, k_sym), log_ll.diff(b_y[k_sym]), modules='sympy')\n",
    "dll_dpz_k = lambdify((a_y, b_y, a_u, b_u, P_z, y, k_sym), log_ll.diff(P_z[k_sym]), modules='sympy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processInput(k):\n",
    "    return dll_da_k(alphas, betas, p_z, y_train, k), dll_db_k(alphas, betas, p_z, y_train, k), dll_dpz_k(alphas, betas, p_z, y_train, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clinton]",
   "language": "python",
   "name": "conda-env-clinton-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
